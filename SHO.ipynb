{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([[1,2],[3,2]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate some data for the simple harmonic oscillator given by\n",
    "$$\\ddot{x} + \\omega^2 x = 0$$\n",
    "but for simplicity we will set $\\omega = 1$.\n",
    "\n",
    "We will do this by simply giving data as pairs of numbers at time $t$ and $t+dt$, the pair being $x(t)$ and $\\dot{x}(t)$, which we will generate knowing the true equation of motion.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sho_data(num_samples=1000, T=20, dt=0.1, omega=1.0):\n",
    "\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Random initial conditions\n",
    "        x0 = np.random.uniform(-1.0, 1.0)\n",
    "        v0 = np.random.uniform(-1.0, 1.0)\n",
    "\n",
    "        # Analytical solution for SHO with omega=1:\n",
    "        # x(t) = x0 cos(t) + (v0) sin(t)\n",
    "        # v(t) = -x0 sin(t) + v0 cos(t)\n",
    "        # If you want general omega, just use cos(omega t), etc.\n",
    "\n",
    "        t_vals = np.arange(T) * dt # Create time points, an array from 0 to T*dt with step dt\n",
    "        x_vals = x0*np.cos(omega*t_vals) + v0*np.sin(omega*t_vals)\n",
    "        v_vals = -x0*np.sin(omega*t_vals)*omega + v0*np.cos(omega*t_vals)*omega\n",
    "        # Note that if omega=1, this simplifies, but let's keep it general.\n",
    "\n",
    "        # Pack into shape (T, 2). Use np.stack but ultimately reshapre using torch.Tensor\n",
    "        seq = np.stack([x_vals, v_vals], axis=1)  # axis=1 means we stack along the columns\n",
    "        \n",
    "        # Next-step targets: shift by 1. We wanto predict the next step given the current step, i.e. go from x(t) to x(t+dt).\n",
    "        # For the last step, let's just copy the last value or set to zeros\n",
    "        seq_target = np.zeros_like(seq) # zeros_like creates an array of zeros with the same shape as seq\n",
    "        seq_target[:-1] = seq[1:]\n",
    "        seq_target[-1] = seq[-1]  # for convenience\n",
    "\n",
    "        X_data.append(seq)\n",
    "        Y_data.append(seq_target)\n",
    "\n",
    "    X_data = np.array(X_data, dtype=np.float32)  # (num_samples, T, 2)\n",
    "    Y_data = np.array(Y_data, dtype=np.float32)  # (num_samples, T, 2)\n",
    "\n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the transformer which we want to train on this data.\n",
    "\n",
    "First, we need to encode position into the network. Our data is necessarily _ordered_ from the past into the future, but the transformer doesn't know this (it processes everything at the same time). To get around this, we build a positional encoding, which tags each chuck of data with a position in the sequence. We can inheret PositionalEncoding from pytorch to do this.\n",
    "\n",
    "Useful torch modules to understand: \n",
    "- .unsqueeze(n), which adds a dimension to a tensor at position n. Basically, adding an index to a tensor at position n. Example: $(T^\\mu)$.unsqueeze(0) $= T^{\\alpha\\mu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=200): # d_model is the number of dimensions in the input, and max_len is the maximum length of the sequence we expect. O've set it to 200 steps by default, but you can change this if you expect longer sequences. \n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model) # initialise a tensor of zeros with shape (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # arange creates a 1D tensor from 0 to max_len, with data type float\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * \n",
    "                             (-np.log(10000.0) / d_model)) # Create a list of numbers starting from 0 to d_model with step 2, multiply by -log(10000)/d_model, and exponentiate. This essentially creates a series of decreasing factors which we use to scale the argument of the sin and cos functions.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # :, 0::2 is fancy python for \"every other column starting from 0\". The general syntax is [start:end:step]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # We want this to be saved along with the model weights, but not trained, so we register it as a buffer. We also unsqueeze to add a batch dimension of 1.\n",
    "\n",
    "    def forward(self, x): # forward might be called on a given tensor, x, and this function returns the tensor with positional encoding added\n",
    "        seq_len = x.size(1) # work out how long the sequence is\n",
    "        return x + self.pe[:, :seq_len] # return the tensor now with new positional encoding added. We slice the positional encoding to be the same length as the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have added positional data, we build the transformer, using the pytorch TransformerModel. For this, we need to think about how we want our transformer to behave. We need to create an embedding layer, basically the layer that our inputs will go into and be transformed into the high-dimensional space that is the encoder. Language models will often use nn.Embedding, but we will use nn.Linear, since our data is made up of continuous variables (position and velocity) rather than e.g. tokens.\n",
    "\n",
    "I've added a _causal mask_, which essentially blocks the transformer from seeing 'future steps', which might have been causing it to make bad predictions. Essentially, the mask is filled with past information and -\\infinity anywhere in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate a causal _mask_ of shape (seq_len, seq_len).\n",
    "    Upper-triangular entries (excluding main diagonal) are set to -inf.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    # Wherever mask == 1, we set to -inf\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model) # This is the input embedding layer. It's a simple linear layer that maps the input dimension to the model dimension. In our case, the input dimension is 2: (x(t), x'(t)) and the model dimension is whatever we choose in our hyperparameters (d_model, e.g. 128).\n",
    "        self.pos_encoder = PositionalEncoding(d_model) # See above\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                   nhead=num_heads,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout,\n",
    "                                                   batch_first=True) # The dropout here is basically a regularisation technique. It randomly sets some of the input units to zero with probability p. This supposedly helps to prevent overfitting. The batch_first=True argument means that the input and output tensors are of shape (batch_size, seq_length, input_dim) rather than the default (seq_length, batch_size, input_dim).\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,\n",
    "                                                         num_layers=num_layers)\n",
    "\n",
    "        # Now map back to 2D (x, v)\n",
    "        self.fc_out = nn.Linear(d_model, input_dim)\n",
    "    \n",
    "    def forward(self, src):\n",
    "\n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        \n",
    "        # Generate the causal mask for this sequence length\n",
    "        seq_len = x.size(1)\n",
    "        mask = generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "\n",
    "        # Pass mask=mask to enforce auto-regressive attention\n",
    "        encoded = self.transformer_encoder(x, mask=mask) # Transformer encoder, with the mask applied. This is where the magic happens, and the model learns to predict the next step in the sequence.\n",
    "\n",
    "        # Map to output dimension. This is the final linear layer that maps the model dimension (e.g. 128) back to the input dimension (2).\n",
    "        out = self.fc_out(encoded) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the main function, generate some data, (optionally) train the model and then compare with some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 0: NVIDIA GeForce RTX 3070\n",
      "Training the model on SHO data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 29/200 [01:18<07:29,  2.63s/epoch, loss=0.001211]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ---------------------------\n",
    "    # Hyperparameters\n",
    "    # ---------------------------\n",
    "    enable_training = False\n",
    "    MODEL_PATH = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"sho_transformer_model.pth\") # Let's keep the model in the same directory as the script for simplicity\n",
    "\n",
    "    # SHO data parameters\n",
    "    num_samples = 10000  # how many sequences\n",
    "    T = 200              # length of each sequence\n",
    "    dt = 0.1            # time step in the SHO solver\n",
    "    standardize = True   # standardize the data or not\n",
    "\n",
    "    # Transformer hyperparams\n",
    "    input_dim = 2       # [x(t), v(t)]\n",
    "    d_model = 128       # embedding dimension\n",
    "    num_heads = 2\n",
    "    num_layers = 4\n",
    "    dim_feedforward = 64\n",
    "    dropout = 0.01 # Randomly drop out 10% of the units\n",
    "    \n",
    "    # Training hyperparams\n",
    "    num_epochs = 200\n",
    "    \n",
    "    X, Y = generate_sho_data(num_samples, T, dt, omega=1)\n",
    "    # X, Y have shape (num_samples, T, 2) => (batch, seq_len, channels=[x,v])\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_tensor = torch.tensor(X)  # (num_samples, T, 2)\n",
    "    Y_tensor = torch.tensor(Y)  # (num_samples, T, 2)\n",
    "\n",
    "    # Optional: standardize or not. Sometimes it helps.\n",
    "\n",
    "    if standardize:\n",
    "        X_mean = X_tensor.mean(dim=(0,1))\n",
    "        X_std = X_tensor.std(dim=(0,1))\n",
    "        X_tensor = (X_tensor - X_mean) / (X_std + 1e-6)\n",
    "        Y_tensor = (Y_tensor - X_mean) / (X_std + 1e-6) # Note we add 1e-6 to avoid division by zero\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Create model\n",
    "    # ---------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel(input_dim, d_model, num_heads, num_layers, dim_feedforward, dropout)\n",
    "    model.to(device)\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Using GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Optionally train\n",
    "    # ---------------------------\n",
    "    if enable_training:\n",
    "        print(\"Training the model on SHO data...\")\n",
    "        dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            dataloader = DataLoader(dataset, batch_size=512, shuffle=True, pin_memory=True) # What's the best batch size for a modern GPU? I've no idea but this works well in testing.\n",
    "        else:\n",
    "            dataloader = DataLoader(dataset, batch_size=8, shuffle=True) # Smaller batch size for CPU training\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) # lr is the learning rate. This is a hyperparameter that you can tune. It's the step size that the optimizer takes when updating the weights.\n",
    "\n",
    "        progress_bar = tqdm(range(num_epochs), desc=\"Training\", unit=\"epoch\")\n",
    "        for epoch in progress_bar:\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_Y = batch_Y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_X)\n",
    "                loss = criterion(output, batch_Y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            progress_bar.set_postfix(loss=f\"{avg_loss:.6f}\")\n",
    "\n",
    "        # Save trained weights\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(\"Training complete. Model saved.\")\n",
    "    else:\n",
    "        # Load a previously trained model\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print(\"Loaded pre-trained model weights.\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 4) Testing/Inference by ROLLING OUT\n",
    "    # -------------------------------------------------\n",
    "    # Let’s pick a random initial condition for demonstration\n",
    "    x0_test = 0.5\n",
    "    v0_test = 1.0\n",
    "    T_test = 100\n",
    "    test_t = np.arange(T_test)*dt\n",
    "\n",
    "    # True solution (for w=1)\n",
    "    true_x = x0_test*np.cos(test_t) + v0_test*np.sin(test_t)\n",
    "    true_v = -x0_test*np.sin(test_t) + v0_test*np.cos(test_t)\n",
    "\n",
    "    # We'll do a \"rollout\": at each step we feed the previous (x, v).\n",
    "    # Start from (x0, v0).\n",
    "    rollout = []\n",
    "    x_curr = x0_test\n",
    "    v_curr = v0_test\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for t_idx in range(T_test):\n",
    "            inp = np.array([[x_curr, v_curr]], dtype=np.float32)  # shape (1,2)\n",
    "            if standardize:\n",
    "                inp = (inp - X_mean.numpy()) / (X_std.numpy() + 1e-6) # Standardize the current input using training mean/std\n",
    "\n",
    "            inp_tensor = torch.tensor(inp, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            # shape (1, seq_len=1, 2)\n",
    "\n",
    "            out_tensor = model(inp_tensor)\n",
    "            # out_tensor shape => (1, 1, 2) => the predicted next [x, v]\n",
    "\n",
    "            pred = out_tensor.squeeze(0).squeeze(0).cpu().numpy()  # shape (2,)\n",
    "            # un-standardize\n",
    "            if standardize:\n",
    "                pred_unscaled = pred*(X_std.numpy() + 1e-6) + X_mean.numpy()\n",
    "            else:\n",
    "                pred_unscaled = pred\n",
    "            # record\n",
    "            rollout.append(pred_unscaled)\n",
    "\n",
    "            # feed the predicted next step in for next iteration\n",
    "            x_curr, v_curr = pred_unscaled\n",
    "\n",
    "    rollout = np.array(rollout)  # shape (T_test, 2)\n",
    "    pred_x = rollout[:, 0]\n",
    "    pred_v = rollout[:, 1]\n",
    "    print(true_x,true_v)\n",
    "    print(pred_x, pred_v)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(test_t, true_x, label='True x(t)', marker='o', markevery=5)\n",
    "    plt.plot(test_t, pred_x, label='Pred x(t)', linestyle='--')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"x(t)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Rollout Prediction: x(t) for SHO (omega=1)\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(test_t, true_v, label='True v(t)', marker='o', markevery=5)\n",
    "    plt.plot(test_t, pred_v, label='Pred v(t)', linestyle='--')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"v(t)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Rollout Prediction: v(t) for SHO (omega=1)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Difference plot for v(t)\n",
    "    diff_v = np.abs(true_v - pred_v)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(test_t, diff_v, label='Absolute Error in v(t)', marker='o', markevery=5, color='red')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Absolute Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Difference Plot: True vs Predicted v(t)\")\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
